key,text
2370475239,transformers.fx.symbolic_trace supports inputs_embeds
2370664486,OOM when loading 300B models with AutoModelForCausalLM.from_pretrained and BitsAndBytesConfig quantization.
2370728606,add warning when using gradient_checkpointing with FSDP full shard
2370963909,[WIP] Add implementation of _extract_fbank_features_batch
2371219776,Allow infer_framework_load_model to use the originally specified config
2371608016,Multi-GPU inference affects LLM's (Llama2-7b-chat-hf) generation
2371774073,[Bug Fix] fix qa pipeline tensor to numpy
2372924504,[QoL] Allow dtype str for torch_dtype arg of from_pretrained
2372942675,[Llama] Conversion: fix and simplify the script!
2373805505,Unable to export Phi-3-vision model to PyTorch exported program
2374347292,Tokenizer discard data that exceed max_length